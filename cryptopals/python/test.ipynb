{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from my_cryptopals_tools import hex_to_base64, xor_hex_strings, xor_byte_strings, chi_squared_score\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSdtIGtpbGxpbmcgeW91ciBicmFpbiBsaWtlIGEgcG9pc29ub3VzIG11c2hyb29t\n"
     ]
    }
   ],
   "source": [
    "hex_string = \"49276d206b696c6c696e6720796f757220627261696e206c696b65206120706f69736f6e6f7573206d757368726f6f6d\"\n",
    "print(hex_to_base64(hex_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'746865206b696420646f6e277420706c6179'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex_string1 = \"1c0111001f010100061a024b53535009181c\"\n",
    "hex_string2 = \"686974207468652062756c6c277320657965\"\n",
    "\n",
    "xor_hex_strings(hex_string1, hex_string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1 in bytes is b'\\x1c\\x01\\x11\\x00\\x1f\\x01\\x01\\x00\\x06\\x1a\\x02KSSP\\t\\x18\\x1c'\n",
      "String 2 in bytes is b\"hit the bull's eye\"\n",
      "Xor in bytes is b\"the kid don't play\"\n",
      "Xor solution in hex is 746865206b696420646f6e277420706c6179\n"
     ]
    }
   ],
   "source": [
    "hex_string1 = \"1c0111001f010100061a024b53535009181c\"\n",
    "hex_string2 = \"686974207468652062756c6c277320657965\"\n",
    "\n",
    "xor_hex_strings(hex_string1, hex_string2)\n",
    "\n",
    "byte_string1 = bytes.fromhex(hex_string1)\n",
    "byte_string2 = bytes.fromhex(hex_string2)\n",
    "\n",
    "print (f'String 1 in bytes is {byte_string1}')\n",
    "print (f'String 2 in bytes is {byte_string2}')\n",
    "print (f'Xor in bytes is {xor_byte_strings(byte_string1, byte_string2)}')\n",
    "print (f'Xor solution in hex is {xor_hex_strings(hex_string1, hex_string2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooking MC's like a pound of bacon\n",
      "score 88 is 61.98055038379752\n",
      "Bnnjhof!LB&r!mhjd!`!qntoe!ng!c`bno\n",
      "score 89 is 201.14471125862227\n",
      "Ammikle\"OA%q\"nkig\"c\"rmwlf\"md\"`caml\n",
      "score 90 is 144.06710815803274\n",
      "@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm\n",
      "score 91 is 175.76283535141496\n",
      "Gkkomjc$IG#w$hmoa$e$tkqj`$kb$fegkj\n",
      "score 92 is 206.2190473203588\n",
      "Fjjnlkb%HF\"v%iln`%d%ujpka%jc%gdfjk\n",
      "score 93 is 212.51754224123897\n",
      "Eiimoha&KE!u&jomc&g&vishb&i`&dgeih\n",
      "score 94 is 128.3412608482029\n",
      "Best string was: \"Cooking MC's like a pound of bacon\"\n",
      "Best char was:  88\n",
      "Best score was:  61.98055038379752\n"
     ]
    }
   ],
   "source": [
    "hex_string = '1b37373331363f78151b7f2b783431333d78397828372d363c78373e783a393b3736'\n",
    "\n",
    "\n",
    "best_score = 9999999.\n",
    "for i in range(88,95):\n",
    "    byte_string1 = bytes.fromhex(hex_string)\n",
    "    byte_string2 = i.to_bytes()\n",
    "\n",
    "    #print (f'Xor in bytes with {i} is {xor_byte_strings(byte_string1, byte_string2)}')\n",
    "    #print (f'Xor in hex is {xor_byte_strings(byte_string1, byte_string2).decode('utf-8')}')\n",
    "    raw_string = xor_byte_strings(byte_string1, byte_string2).decode('latin-1')\n",
    "    print (raw_string)\n",
    "    score = chi_squared_score(raw_string)\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_char = i\n",
    "        best_string = raw_string\n",
    "    print (f'score {i} is {score}')\n",
    "\n",
    "print (f'Best string was: \"{best_string}\"')\n",
    "print ('Best char was: ', best_char)\n",
    "print ('Best score was: ', best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The more likely English sentence is: '@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm'\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def score_string_by_letter_frequency_improved(string: str, letter_freqs: dict) -> float:\n",
    "    \"\"\"\n",
    "    Scores a string based on letter frequency, normalizes by length, and penalizes repetition.\n",
    "\n",
    "    :param string: The string to score.\n",
    "    :param letter_freqs: A dictionary of letter frequencies.\n",
    "    :return: A likelihood score for the string.\n",
    "    \"\"\"\n",
    "    string = string.lower()  # Normalize to lowercase\n",
    "    total_score = 0.0\n",
    "    char_count = Counter(string)  # Count occurrences of each character\n",
    "    length = len(string)\n",
    "    \n",
    "    if length == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    # Add letter frequency scores for valid characters\n",
    "    for char, count in char_count.items():\n",
    "        if char in letter_freqs:\n",
    "            total_score += letter_freqs[char] * count\n",
    "\n",
    "    # Normalize the score by length\n",
    "    normalized_score = total_score / length\n",
    "\n",
    "    # Penalize low diversity using Shannon entropy\n",
    "    probabilities = [count / length for count in char_count.values()]\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "    # Combine normalized score with entropy (weighted for balance)\n",
    "    final_score = normalized_score * 0.7 + entropy * 0.3\n",
    "\n",
    "    return final_score\n",
    "\n",
    "def compare_strings_by_likelihood_improved(string1: str, string2: str, letter_freqs: dict) -> str:\n",
    "    \"\"\"\n",
    "    Compares two strings to determine which is more likely an English sentence.\n",
    "\n",
    "    :param string1: The first string.\n",
    "    :param string2: The second string.\n",
    "    :param letter_freqs: A dictionary of letter frequencies.\n",
    "    :return: The string more likely to be English text.\n",
    "    \"\"\"\n",
    "    score1 = score_string_by_letter_frequency_improved(string1, letter_freqs)\n",
    "    score2 = score_string_by_letter_frequency_improved(string2, letter_freqs)\n",
    "\n",
    "    return string1 if score1 > score2 else string2\n",
    "\n",
    "# Example usage\n",
    "letter_freqs = {'e': 0.112, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "                't': 0.0665, 's': 0.0573, 'l': 0.0548}\n",
    "\n",
    "string1 = \"Cooking MC's like a pound of bacon\"\n",
    "string2 = \"@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm\"\n",
    "\n",
    "more_likely = compare_strings_by_likelihood_improved(string1, string2, letter_freqs)\n",
    "print(f\"The more likely English sentence is: '{more_likely}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most English-like string: Cooking MC's like a pound of bacon\n",
      "Scores: {\"Cooking MC's like a pound of bacon\": 14.067738958676314, '@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm': 29.311344507822206, 'eeeeeeeeeeeeeeeeeeee': 155.89946953405018, 'kjhifykboipjepbudoipo': 21.320287079675367}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def chi_squared_score(s, letter_freqs):\n",
    "    # Normalize the string: lowercase and keep only alphabetic characters\n",
    "    s = s.lower()\n",
    "    s = ''.join(filter(str.isalpha, s))\n",
    "    total_chars = len(s)\n",
    "    if total_chars == 0:\n",
    "        return float('inf')  # Empty strings are penalized\n",
    "\n",
    "    # Count observed frequencies of each letter in the string\n",
    "    observed_counts = Counter(s)\n",
    "\n",
    "    # Calculate chi-squared statistic\n",
    "    chi_squared = 0\n",
    "    for letter, expected_freq in letter_freqs.items():\n",
    "        observed = observed_counts.get(letter, 0)\n",
    "        expected = expected_freq * total_chars\n",
    "        chi_squared += (observed - expected) ** 2 / expected if expected > 0 else 0\n",
    "\n",
    "    return chi_squared\n",
    "\n",
    "def find_most_english_like(strings, letter_freqs):\n",
    "    # Score each string using chi-squared statistic (lower is better)\n",
    "    scores = {s: chi_squared_score(s, letter_freqs) for s in strings}\n",
    "    return min(scores, key=scores.get), scores\n",
    "\n",
    "# Test inputs\n",
    "strings = [\n",
    "    \"Cooking MC's like a pound of bacon\",\n",
    "    \"@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm\",\n",
    "    \"eeeeeeeeeeeeeeeeeeee\",\n",
    "    \"kjhifykboipjepbudoipo\"\n",
    "]\n",
    "\n",
    "letter_freqs = {\n",
    "    'e': 0.1116, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "    't': 0.0665, 's': 0.0573, 'l': 0.0548, 'c': 0.0453, 'u': 0.0363,\n",
    "    'd': 0.0384, 'p': 0.0316, 'm': 0.0301, 'h': 0.0300, 'g': 0.0247\n",
    "}\n",
    "\n",
    "most_english_like, all_scores = find_most_english_like(strings, letter_freqs)\n",
    "print(\"Most English-like string:\", most_english_like)\n",
    "print(\"Scores:\", all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most English-like string by KL divergence: eeeeeeeeeeeeeeeeeeee\n",
      "KL divergence scores: {\"Cooking MC's like a pound of bacon\": 7.948107645911927, '@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm': 9.19850376242986, 'eeeeeeeeeeeeeeeeeeee': 3.1635910677202617, 'kjhifykboipjepbudoipo': 11.859082152195056}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def kl_divergence_score(s, letter_freqs):\n",
    "    # Normalize the string: lowercase and keep only alphabetic characters\n",
    "    s = s.lower()\n",
    "    s = ''.join(filter(str.isalpha, s))\n",
    "    total_chars = len(s)\n",
    "    if total_chars == 0:\n",
    "        return float('inf')  # Empty strings are penalized\n",
    "\n",
    "    # Count occurrences of each letter in the string\n",
    "    observed_counts = Counter(s)\n",
    "    \n",
    "    # Calculate observed probabilities\n",
    "    observed_probs = {char: count / total_chars for char, count in observed_counts.items()}\n",
    "    \n",
    "    # Calculate KL divergence\n",
    "    kl_divergence = 0\n",
    "    for char, observed_prob in observed_probs.items():\n",
    "        expected_prob = letter_freqs.get(char, 1e-10)  # Use a small value to avoid log(0)\n",
    "        kl_divergence += observed_prob * math.log2(observed_prob / expected_prob)\n",
    "    \n",
    "    return kl_divergence\n",
    "\n",
    "def find_most_english_like_kl(strings, letter_freqs):\n",
    "    # Score each string using KL divergence (lower is better)\n",
    "    scores = {s: kl_divergence_score(s, letter_freqs) for s in strings}\n",
    "    return min(scores, key=scores.get), scores\n",
    "\n",
    "# Test inputs\n",
    "strings = [\n",
    "    \"Cooking MC's like a pound of bacon\",\n",
    "    \"@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm\",\n",
    "    \"eeeeeeeeeeeeeeeeeeee\",\n",
    "    \"kjhifykboipjepbudoipo\"\n",
    "]\n",
    "\n",
    "letter_freqs = {\n",
    "    'e': 0.1116, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "    't': 0.0665, 's': 0.0573, 'l': 0.0548, 'c': 0.0453, 'u': 0.0363,\n",
    "    'd': 0.0384, 'p': 0.0316, 'm': 0.0301, 'h': 0.0300, 'g': 0.0247\n",
    "}\n",
    "\n",
    "most_english_like, all_scores = find_most_english_like_kl(strings, letter_freqs)\n",
    "print(\"Most English-like string by KL divergence:\", most_english_like)\n",
    "print(\"KL divergence scores:\", all_scores)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely English sentence is: '--)+,%b e1b.+)'b#b2-7,&b-$b #!-,'\n",
      "Chi-squared scores for each string:\n",
      "'Cooking MC's like a pound of bacon': 14.569451995548476\n",
      "'@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm': 44.6940863639843\n",
      "'eeeeeeeeeeeeeeeeeeee': 159.89946953405018\n",
      "'kjhifykboipjepbudoipo': 35.5822483594756\n",
      "'Bnnjhof!LB&r!mhjd!`!qntoe!ng!c`bno': 21.00841194715189\n",
      "'--)+,%b e1b.+)'b#b2-7,&b-$b #!-,': 6.756724492234169\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def chi_squared_score(s, letter_freqs):\n",
    "    \"\"\"\n",
    "    Computes the chi-squared score for how closely the letter distribution of a string matches expected frequencies.\n",
    "    \"\"\"\n",
    "    # Normalize the input string and count letters\n",
    "    s = s.lower()\n",
    "    counts = Counter(char for char in s if char in letter_freqs)\n",
    "    total_letters = sum(counts.values())\n",
    "    \n",
    "    if total_letters == 0:\n",
    "        return float('inf')  # No valid letters, penalize heavily\n",
    "    \n",
    "    chi_squared = 0\n",
    "    for letter, expected_freq in letter_freqs.items():\n",
    "        observed = counts.get(letter, 0)\n",
    "        expected = total_letters * expected_freq\n",
    "        chi_squared += (observed - expected) ** 2 / expected if expected > 0 else 0\n",
    "\n",
    "    return chi_squared\n",
    "\n",
    "def find_most_english_string(strings, letter_freqs):\n",
    "    \"\"\"\n",
    "    Finds the string that is most likely an English sentence based on chi-squared scoring.\n",
    "    Lower scores indicate a closer match to English letter frequencies.\n",
    "    \"\"\"\n",
    "    scores = {s: chi_squared_score(s, letter_freqs) for s in strings}\n",
    "    return min(scores, key=scores.get), scores\n",
    "\n",
    "# Provided strings and letter frequencies\n",
    "strings = [\n",
    "    \"Cooking MC's like a pound of bacon\",\n",
    "    \"@llhjmd#N@$p#ojhf#b#slvmg#le#ab`lm\",\n",
    "    \"eeeeeeeeeeeeeeeeeeee\",\n",
    "    \"kjhifykboipjepbudoipo\",\n",
    "    \"Bnnjhof!LB&r!mhjd!`!qntoe!ng!c`bno\",\n",
    "    \"--)+,%b e1b.+)'b#b2-7,&b-$b #!-,\"\n",
    "]\n",
    "\n",
    "letter_freqs = {'e': 0.1116, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "                't': 0.0665, 's': 0.0573, 'l': 0.0548, 'c': 0.0453, 'u': 0.0363,\n",
    "                'd': 0.0384, 'p': 0.0316, 'm': 0.0301, 'h': 0.0300, 'g': 0.0247, ' ': 0.2}\n",
    "\n",
    "# Find the most likely English sentence\n",
    "most_english, scores = find_most_english_string(strings, letter_freqs)\n",
    "\n",
    "print(f\"The most likely English sentence is: '{most_english}'\")\n",
    "print(\"Chi-squared scores for each string:\")\n",
    "for s, score in scores.items():\n",
    "    print(f\"'{s}': {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's pretend we have five strings:\n",
    "\n",
    "string1 = \"Cooking MC's like a pound of bacon\"\n",
    "string2 = \"@llhjmd#N@$p#ojhf#b#slvmg#le#ablm\"\n",
    "string3 = \"eeeeeeeeeeeeeeeeeeeeeee\"\n",
    "string4 = \"kjhifykboipjepbudoipo\"\n",
    "string5 = \"Bnnjhof!LB&r!mhjd!!qntoe!ng!c`bno\"\n",
    "string6 = \"--)+,%b e1b.+)'b#b2-7,&b-$b #!-,\"\n",
    "\n",
    "How do we write a python function that determines which of these strings is most likely to be a real english sentence using the letter frequencies to score them:\n",
    "\n",
    "letter_freqs = {'e': 0.1116, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "                    't': 0.0665, 's': 0.0573, 'l': 0.0548, 'c': 0.0453, 'u': 0.0363,\n",
    "                    'd': 0.0384, 'p': 0.0316, 'm': 0.0301, 'h': 0.0300, 'g': 0.0247, ' ': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely English sentence: Cooking MC's like a pound of bacon\n",
      "Chi-squared scores: {\"Cooking MC's like a pound of bacon\": 93.50934564365471, '@llhjmd#N@$p#ojhf#b#slvmg#le#ablm': 191.61574146023435, 'eeeeeeeeeeeeeeeeeeeeeee': 183.8843899641577, 'kjhifykboipjepbudoipo': 105.52028707967537, 'Bnnjhof!LB&r!mhjd!!qntoe!ng!c`bno': 221.35628349433256, \"--)+,%b e1b.+)'b#b2-7,&b-$b #!-,\": 318.00581792114696}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def chi_squared_score(string, letter_freqs):\n",
    "    # Normalize: Convert to lowercase and count all characters\n",
    "    normalized = ''.join(c.lower() if c.lower() in letter_freqs else '#' for c in string)\n",
    "    \n",
    "    # Count occurrences of each character\n",
    "    total_chars = len(normalized)\n",
    "    if total_chars == 0:  # Avoid division by zero\n",
    "        return float('inf')\n",
    "    \n",
    "    observed_counts = Counter(normalized)\n",
    "    \n",
    "    # Calculate chi-squared score\n",
    "    chi_squared = 0\n",
    "    for char, expected_freq in letter_freqs.items():\n",
    "        observed = observed_counts.get(char, 0)\n",
    "        expected = total_chars * expected_freq\n",
    "        chi_squared += ((observed - expected) ** 2) / expected if expected > 0 else 0\n",
    "\n",
    "    # Add penalty for uncommon characters\n",
    "    uncommon_count = observed_counts.get('#', 0)\n",
    "    chi_squared += uncommon_count * 10  # Large penalty for each uncommon character\n",
    "    \n",
    "    return chi_squared\n",
    "\n",
    "def most_likely_sentence(strings, letter_freqs):\n",
    "    # Score each string using the chi-squared method\n",
    "    scores = {string: chi_squared_score(string, letter_freqs) for string in strings}\n",
    "    best_string = min(scores, key=scores.get)  # Lower chi-squared is better\n",
    "    return best_string, scores\n",
    "\n",
    "# Letter frequencies\n",
    "letter_freqs = {\n",
    "    'e': 0.1116, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "    't': 0.0665, 's': 0.0573, 'l': 0.0548, 'c': 0.0453, 'u': 0.0363,\n",
    "    'd': 0.0384, 'p': 0.0316, 'm': 0.0301, 'h': 0.0300, 'g': 0.0247, \n",
    "    ' ': 0.2  # Spaces are very common in English sentences\n",
    "}\n",
    "\n",
    "# Strings to analyze\n",
    "strings = [\n",
    "    \"Cooking MC's like a pound of bacon\",\n",
    "    \"@llhjmd#N@$p#ojhf#b#slvmg#le#ablm\",\n",
    "    \"eeeeeeeeeeeeeeeeeeeeeee\",\n",
    "    \"kjhifykboipjepbudoipo\",\n",
    "    \"Bnnjhof!LB&r!mhjd!!qntoe!ng!c`bno\",\n",
    "    \"--)+,%b e1b.+)'b#b2-7,&b-$b #!-,\"\n",
    "]\n",
    "\n",
    "# Get the most likely sentence\n",
    "best_string, scores = most_likely_sentence(strings, letter_freqs)\n",
    "\n",
    "print(f\"Most likely English sentence: {best_string}\")\n",
    "print(f\"Chi-squared scores: {scores}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely English sentence: eeeeeeeeeeeeeeeeeeeeeee\n",
      "Entropy scores: {\"Cooking MC's like a pound of bacon\": 5.205421462464042, '@llhjmd#N@$p#ojhf#b#slvmg#le#ablm': 7.1320074015606725, 'eeeeeeeeeeeeeeeeeeeeeee': 3.1635910677202617, 'kjhifykboipjepbudoipo': 6.42794072639369, 'Bnnjhof!LB&r!mhjd!!qntoe!ng!c`bno': 7.739125069761785, \"--)+,%b e1b.+)'b#b2-7,&b-$b #!-,\": 9.275474734771734}\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "from collections import Counter\n",
    "\n",
    "def entropy_score(s, letter_freqs):\n",
    "    # Normalize string: convert to lowercase and filter relevant characters\n",
    "    #normalized = ''.join(c.lower() for c in string if c.lower() in letter_freqs)\n",
    "    normalized = ''.join(c.lower() if c.lower() in letter_freqs else '#' for c in s)\n",
    "    \n",
    "    # Count occurrences of each letter\n",
    "    total_chars = len(normalized)\n",
    "    if total_chars == 0:  # Avoid division by zero\n",
    "        return float('inf')\n",
    "    \n",
    "    observed_counts = Counter(normalized)\n",
    "    \n",
    "    # Calculate observed frequencies\n",
    "    observed_freqs = {char: count / total_chars for char, count in observed_counts.items()}\n",
    "    \n",
    "    # Calculate entropy deviation\n",
    "    entropy = 0\n",
    "    for char, expected_freq in letter_freqs.items():\n",
    "        observed_freq = observed_freqs.get(char, 0)\n",
    "        if observed_freq > 0:\n",
    "            entropy += -observed_freq * log2(expected_freq)\n",
    "    \n",
    "    # Penalize strings with missing or uncommon characters\n",
    "    uncommon_count = total_chars - sum(observed_counts[char] for char in letter_freqs if char in observed_counts)\n",
    "    entropy += uncommon_count * 10  # Large penalty for uncommon characters\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def most_likely_sentence(strings, letter_freqs):\n",
    "    # Score each string using the entropy method\n",
    "    scores = {string: entropy_score(string, letter_freqs) for string in strings}\n",
    "    best_string = min(scores, key=scores.get)  # Lower entropy is better\n",
    "    return best_string, scores\n",
    "\n",
    "# Letter frequencies\n",
    "letter_freqs = {\n",
    "    'e': 0.1116, 'a': 0.0850, 'r': 0.0758, 'i': 0.0754, 'o': 0.0716, \n",
    "    't': 0.0665, 's': 0.0573, 'l': 0.0548, 'c': 0.0453, 'u': 0.0363,\n",
    "    'd': 0.0384, 'p': 0.0316, 'm': 0.0301, 'h': 0.0300, 'g': 0.0247, \n",
    "    ' ': 0.2, '#': 0.001  # Spaces are very common in English sentences\n",
    "}\n",
    "\n",
    "# Strings to analyze\n",
    "strings = [\n",
    "    \"Cooking MC's like a pound of bacon\",\n",
    "    \"@llhjmd#N@$p#ojhf#b#slvmg#le#ablm\",\n",
    "    \"eeeeeeeeeeeeeeeeeeeeeee\",\n",
    "    \"kjhifykboipjepbudoipo\",\n",
    "    \"Bnnjhof!LB&r!mhjd!!qntoe!ng!c`bno\",\n",
    "    \"--)+,%b e1b.+)'b#b2-7,&b-$b #!-,\"\n",
    "]\n",
    "\n",
    "# Get the most likely sentence\n",
    "best_string, scores = most_likely_sentence(strings, letter_freqs)\n",
    "\n",
    "print(f\"Most likely English sentence: {best_string}\")\n",
    "print(f\"Entropy scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.643856189774724"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
